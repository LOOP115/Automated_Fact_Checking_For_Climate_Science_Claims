{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP90042 Project 2023 - Automated Fact Checking For Climate Science Claims\n",
    "\n",
    "Author: Jiahao Chen\n",
    "\n",
    "Student ID: 1118749\n",
    "\n",
    "The script is capable of running on Colab using the basic T4 GPU. If running out of CUDA memory, please restart the kernel and run all. Previous progress are saved for fast recovery."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install and import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch torchvision transformers \n",
    "!pip install pandas numpy sklearn nltk\n",
    "!pip install ipywidgets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab, uncomment the following lines\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "data_dir = \"data/\"\n",
    "# Change if on Colab\n",
    "# data_dir = \"drive/MyDrive/data/\"\n",
    "\n",
    "outputs_path = \"outputs/\"\n",
    "prediction_path = \"prediction/\"\n",
    "\n",
    "train_path = f\"{data_dir}train-claims.json\"\n",
    "dev_path = f\"{data_dir}dev-claims.json\"\n",
    "evidence_path = f\"{data_dir}evidence.json\"\n",
    "\n",
    "token_len = 256\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "gpu = 0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: data/\n",
      "Directory already exists: outputs/\n",
      "Directory already exists: prediction/\n"
     ]
    }
   ],
   "source": [
    "# Check if file or directory exists\n",
    "def check_file(path):\n",
    "    if os.path.exists(path):\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"{path} does not exist.\")\n",
    "        return False\n",
    "\n",
    "def check_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Created directory: {path}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {path}\")\n",
    "\n",
    "check_dir(data_dir)\n",
    "check_dir(outputs_path)\n",
    "check_dir(prediction_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section I: Evidence Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load claims, related evidences and labels\n",
    "# If label is False, return only claims and claim ids\n",
    "def load_data(path, label=False):\n",
    "    claimid_list = []\n",
    "    claim_list = []\n",
    "    if label:\n",
    "        evidences_list = []\n",
    "        label_list = []\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data:\n",
    "        claimid_list.append(item)\n",
    "        claim_list.append(data[item]['claim_text'])\n",
    "        if label:\n",
    "            evidences_list.append(data[item]['evidences'])\n",
    "            label_list.append(data[item]['claim_label'])\n",
    "    \n",
    "    if label:\n",
    "        return claimid_list, claim_list, evidences_list, label_list\n",
    "    return claimid_list, claim_list\n",
    "\n",
    "# Load evidences\n",
    "def load_evidence(path):\n",
    "    evidence_list = []\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    for item in data:\n",
    "        evidence_list.append(data[item])\n",
    "    return evidence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_claim_ids, train_claims, train_evidences, train_labels = load_data(train_path, label=True)\n",
    "dev_claim_ids, dev_claims, dev_evidences, dev_labels = load_data(dev_path, label=True)\n",
    "evidence_src = load_evidence(evidence_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean texts using nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "clean_pattern = r'[^a-zA-Z0-9\\s]+'\n",
    "\n",
    "def text_clean(text):\n",
    "    clean_text = re.sub(clean_pattern, '', text)\n",
    "    words = nltk.word_tokenize(clean_text.lower())\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    clean_text = ' '.join(words)\n",
    "    return clean_text\n",
    "\n",
    "# Clean all evidences\n",
    "def evidence_clean(texts):\n",
    "    clean_texts = []\n",
    "    num_texts = len(texts)\n",
    "\n",
    "    print(\"Cleaning texts ...\")\n",
    "    pbar = tqdm(total=num_texts, dynamic_ncols=True, miniters=10000)\n",
    "    for text in texts:\n",
    "        clean_text = text_clean(text)\n",
    "        clean_texts.append(clean_text)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    with open(outputs_path + 'clean_evidence.json', 'w') as f:\n",
    "        json.dump(clean_texts, f)\n",
    "    print(f\"Saved to {outputs_path}clean_evidence.json\")\n",
    "    # return clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from outputs/clean_evidence.json\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned evidences\n",
    "if not check_file(f\"{outputs_path}clean_evidence.json\"):\n",
    "    evidence_clean(evidence_src) \n",
    "with open(outputs_path + 'clean_evidence.json', 'r') as f:\n",
    "    clean_evidence_src = json.load(f)\n",
    "    print(f\"Loaded from {outputs_path}clean_evidence.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute jaccard similarity between two texts\n",
    "def jaccard_similarity(s1, s2):\n",
    "    set1 = set(s1.split())\n",
    "    set2 = set(s2.split())\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    similarity = len(intersection) / len(union)\n",
    "    return similarity\n",
    "\n",
    "# Use Jaccard similarity to filter evidences\n",
    "def jaccard_filter(claim, k=100):\n",
    "    clean_claim = text_clean(claim)\n",
    "    res = []\n",
    "    for i, ev in enumerate(clean_evidence_src):\n",
    "        res.append((i, jaccard_similarity(clean_claim, ev)))\n",
    "    return sorted(res, key = lambda x: x[1], reverse=True)[:k]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform TF-IDF on given texts\n",
    "def tfidf_evidence(texts):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectors = tfidf_vectorizer.fit_transform(texts)\n",
    "    return tfidf_vectors, tfidf_vectorizer\n",
    "\n",
    "# Cosine similarity between TF-IDF vectors of claim and evidence\n",
    "def tfidf_similarity(claim, ev_id):\n",
    "    evidence = clean_evidence_src[ev_id]\n",
    "    claim_vector = ev_tfidf_vectorizer.transform([claim])\n",
    "    evidence_vector = ev_tfidf_vectorizer.transform([evidence])\n",
    "    similarity = cosine_similarity(claim_vector, evidence_vector)\n",
    "    return similarity.item()\n",
    "\n",
    "# Use TF-IDF similarity to filter evidences\n",
    "def tfidf_filter(claim, k=100):\n",
    "    claim = text_clean(claim)\n",
    "    claim_vector = ev_tfidf_vectorizer.transform([claim])\n",
    "    similarities = cosine_similarity(claim_vector, ev_tfidf_vectors)\n",
    "    top_k_indices = np.argsort(similarities, axis=-1)[:, -k:].flatten()\n",
    "    top_k_scores = np.sort(similarities, axis=-1)[:, -k:].flatten()\n",
    "    return list(zip(top_k_indices, top_k_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectors generated.\n"
     ]
    }
   ],
   "source": [
    "# Perform TF-IDF on all evidences\n",
    "ev_tfidf_vectors, ev_tfidf_vectorizer = tfidf_evidence(clean_evidence_src)\n",
    "print(\"TF-IDF vectors generated.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "# Generate embeddings for evidences using DistilBERT\n",
    "def get_text_embedding(evidences, model, tokenizer, batch_size=64, show=True):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    batches = [evidences[i:i+batch_size] for i in range(0, len(evidences), batch_size)]\n",
    "    pbar = tqdm(total=len(batches), dynamic_ncols=True, miniters=1000) if show else None\n",
    "\n",
    "    for batch in batches:\n",
    "        tokenized = tokenizer.batch_encode_plus(batch, padding=True, truncation=True, max_length=token_len, return_tensors='pt')\n",
    "        input_ids = tokenized['input_ids']\n",
    "        attention_masks = tokenized['attention_mask']\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        \n",
    "        # Compute the embeddings for the batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_masks)\n",
    "            # batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "            batch_embeddings = outputs[0][:, 0, :].cpu().numpy()\n",
    "        \n",
    "        embeddings.append(batch_embeddings)\n",
    "        pbar.update(1) if show else None\n",
    "    pbar.close() if show else None\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from outputs/ev_distilbert.npy\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings for all evidences\n",
    "ev_embeddings = None\n",
    "if not check_file(f\"{outputs_path}ev_distilbert.npy\"):\n",
    "    print(\"Generating embeddings for evidences using DistilBERT ...\")\n",
    "    ev_embeddings = get_text_embedding(evidence_src, distilbert_model, distilbert_tokenizer)\n",
    "    np.save(outputs_path + \"ev_distilbert.npy\", ev_embeddings)\n",
    "else:\n",
    "    ev_embeddings = np.load(outputs_path + \"ev_distilbert.npy\")\n",
    "    print(f\"Loaded from {outputs_path}ev_distilbert.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter evidences based on cosine similarity between DistilBERT embeddings of claim and evidence\n",
    "def distilbert_filter(claim, k=100):\n",
    "    claim_embedding = get_text_embedding([claim], distilbert_model, distilbert_tokenizer, show=False)\n",
    "    similarities = cosine_similarity(claim_embedding, ev_embeddings)\n",
    "    top_k_indices = np.argsort(similarities, axis=-1)[:, -k:].flatten()\n",
    "    top_k_scores = np.sort(similarities, axis=-1)[:, -k:].flatten()\n",
    "    return list(zip(top_k_indices, top_k_scores))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on filters (Jaccard, TF-IDF, DistilBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalutate performance of different filters on a claim\n",
    "# k: number of evidences to be retrieved\n",
    "def eval_filter(index, filter_func, k=100, show=True):\n",
    "    claim = dev_claims[index]\n",
    "    truths = dev_evidences[index]\n",
    "    evidences = [item[0] for item in filter_func(claim, k)]\n",
    "    t = 0\n",
    "    f = 0\n",
    "    for truth in truths:\n",
    "        if int(truth[9:]) in evidences:\n",
    "            t += 1\n",
    "            print(f\"In: {truth}\") if show else None\n",
    "        else:\n",
    "            f += 1\n",
    "            print(f\"Out: {truth}\") if show else None\n",
    "    print(f\"In: {t}, Out: {f}\") if show else None\n",
    "    return t, f\n",
    "\n",
    "# Evalutate performance of different filters on all claims in dev set\n",
    "def eval_filter_dev(filter_func, k=100):\n",
    "    t = 0\n",
    "    f = 0\n",
    "    pbar = tqdm(total=len(dev_claims), dynamic_ncols=True)\n",
    "    for i in range(len(dev_claims)):\n",
    "        t_, f_ = eval_filter(i, filter_func, k, False)\n",
    "        t += t_\n",
    "        f += f_\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    print(f\"In: {t}, Out: {f}, Total: {t+f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_filter_dev(jaccard_filter, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_filter_dev(tfidf_filter, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_filter_dev(distilbert_filter, 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT for evidence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample n negative cases\n",
    "num_evidences = len(evidence_src) - 1\n",
    "def get_rand_negative_ids(evidence_ids, n):\n",
    "    res = []\n",
    "    for i in range(n):\n",
    "        temp_id = random.randint(0, num_evidences)\n",
    "        while temp_id in evidence_ids or temp_id in res:\n",
    "            temp_id = random.randint(0, num_evidences)\n",
    "        res.append(temp_id)\n",
    "    return res\n",
    "\n",
    "# Sample n negative cases from evidences that have highest similarity with the claim\n",
    "def get_hard_negative_ids(claim, evidence_ids, filter_func, n):\n",
    "    res = []\n",
    "    similar_ids = filter_func(claim, 50)\n",
    "    cnt = 0\n",
    "    for i in range(n):\n",
    "        temp_id = similar_ids[cnt][0]\n",
    "        while temp_id in evidence_ids:\n",
    "            cnt += 1\n",
    "            temp_id = similar_ids[cnt][0]\n",
    "        res.append(temp_id)\n",
    "        cnt += 1\n",
    "    return res\n",
    "\n",
    "# Pair claim with evidences and labels\n",
    "def pair_claim_evidence(claims, evs, hard=False, show=False):\n",
    "    claim_list = []\n",
    "    ev_list = []\n",
    "    labels = []\n",
    "    \n",
    "    pbar = tqdm(total=len(claims), dynamic_ncols=True) if show else None\n",
    "    for i, claim in enumerate(claims):\n",
    "        # Add positive cases\n",
    "        raw_evidence_ids = evs[i]\n",
    "        evidence_ids = []\n",
    "        for num in raw_evidence_ids:\n",
    "            evidence_id = int(num[9:])\n",
    "            evidence_ids.append(evidence_id)\n",
    "            evidence_text = evidence_src[evidence_id]\n",
    "            claim_list.append(claim)\n",
    "            ev_list.append(evidence_text)\n",
    "            labels.append(1)\n",
    "        \n",
    "        # Add negative cases\n",
    "        num_positive = len(evidence_ids)\n",
    "        if hard:\n",
    "            negative_ids = get_hard_negative_ids(claim, evidence_ids, tfidf_filter, num_positive)\n",
    "        else:\n",
    "            negative_ids = get_rand_negative_ids(evidence_ids, num_positive)\n",
    "        for num in negative_ids:\n",
    "            evidence_text = evidence_src[num]\n",
    "            claim_list.append(claim)\n",
    "            ev_list.append(evidence_text)\n",
    "            labels.append(0)\n",
    "        \n",
    "        pbar.update(1) if show else None\n",
    "    pbar.close() if show else None\n",
    "    return pd.DataFrame({'Claim': claim_list, 'Evidence': ev_list, 'Label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and dev sets using random sampling\n",
    "train_evcls = pair_claim_evidence(train_claims, train_evidences)\n",
    "dev_evcls = pair_claim_evidence(dev_claims, dev_evidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and dev sets (random sampling)\n",
    "train_evcls_f = pd.merge(train_evcls, dev_evcls, on=['Claim', 'Evidence', 'Label'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and dev sets using hard negative sampling\n",
    "train_evcls_hard = None\n",
    "dev_evcls_hard = None\n",
    "train_evcls_hard_data = \"evcls_hard_train.csv\"\n",
    "dev_evcls_hard_data = \"evcls_hard_dev.csv\"\n",
    "\n",
    "if not check_file(f\"{outputs_path}{train_evcls_hard_data}\"):\n",
    "    train_evcls_hard = pair_claim_evidence(train_claims, train_evidences, hard=True, show=True)\n",
    "    train_evcls_hard.to_csv(f\"{outputs_path}{train_evcls_hard_data}\", index=False)\n",
    "    print(f\"Saved to {outputs_path}{train_evcls_hard_data}\")\n",
    "else:\n",
    "    train_evcls_hard = pd.read_csv(f\"{outputs_path}{train_evcls_hard_data}\")\n",
    "\n",
    "if not check_file(f\"{outputs_path}{dev_evcls_hard_data}\"):\n",
    "    dev_evcls_hard = pair_claim_evidence(dev_claims, dev_evidences, hard=True, show=True)\n",
    "    dev_evcls_hard.to_csv(f\"{outputs_path}{dev_evcls_hard_data}\", index=False)\n",
    "    print(f\"Saved to {outputs_path}{dev_evcls_hard_data}\")\n",
    "else:\n",
    "    dev_evcls_hard = pd.read_csv(f\"{outputs_path}{dev_evcls_hard_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and dev sets (hard negative sampling)\n",
    "train_evcls_hard_f = pd.merge(train_evcls_hard, dev_evcls_hard, on=['Claim', 'Evidence', 'Label'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to BERT input format\n",
    "def convert_input(claim, evidence, tokenizer, maxlen):\n",
    "    claim_tokens = tokenizer.tokenize(claim)\n",
    "    evidence_tokens = tokenizer.tokenize(evidence)\n",
    "\n",
    "    while len(claim_tokens) + len(evidence_tokens) > maxlen - 3:\n",
    "        if len(claim_tokens) > len(evidence_tokens):\n",
    "            claim_tokens.pop()\n",
    "        else:\n",
    "            evidence_tokens.pop()\n",
    "\n",
    "    tokens = ['[CLS]'] + evidence_tokens + ['[SEP]'] + claim_tokens + ['[SEP]']\n",
    "    if len(tokens) < maxlen:\n",
    "        tokens = tokens + ['[PAD]' for _ in range(maxlen - len(tokens))]\n",
    "\n",
    "    attn_mask = [0 if token == '[PAD]' else 1 for token in tokens]\n",
    "    seg_ids = [0] * (len(evidence_tokens) + 2) + [1] * (maxlen - len(evidence_tokens) - 2)\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    tokens_ids_t = torch.tensor(tokens_ids)\n",
    "    attn_mask_t = torch.tensor(attn_mask)\n",
    "    seg_ids_t   = torch.tensor(seg_ids)\n",
    "\n",
    "    return tokens_ids_t, attn_mask_t, seg_ids_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for evidence classification\n",
    "class EVCLSDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, datasrc, maxlen):\n",
    "        self.data = datasrc\n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.maxlen = maxlen\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        claim = self.data.loc[index, 'Claim']\n",
    "        evidence = self.data.loc[index, 'Evidence']\n",
    "        label = self.data.loc[index, 'Label']\n",
    "\n",
    "        tokens_ids_t, attn_mask_t, seg_ids_t = convert_input(claim, evidence, self.tokenizer, self.maxlen)\n",
    "        return tokens_ids_t, attn_mask_t, seg_ids_t, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and dev data loader (random sampling)\n",
    "train_evcls_set = EVCLSDataset(train_evcls, token_len)\n",
    "dev_evcls_set = EVCLSDataset(dev_evcls, token_len)\n",
    "train_evcls_loader = DataLoader(train_evcls_set, batch_size = 32, shuffle=True)\n",
    "dev_evcls_loader = DataLoader(dev_evcls_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and dev data loader (hard negative sampling)\n",
    "train_evcls_hard_set = EVCLSDataset(train_evcls_hard, token_len)\n",
    "dev_evcls_hard_set = EVCLSDataset(dev_evcls_hard, token_len)\n",
    "train_evcls_hard_loader = DataLoader(train_evcls_hard_set, batch_size = 32, shuffle=True)\n",
    "dev_evcls_hard_loader = DataLoader(dev_evcls_hard_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader for merged dataset (random sampling)\n",
    "train_evcls_f_set = EVCLSDataset(train_evcls_f, token_len)\n",
    "train_evcls_f_loader = DataLoader(train_evcls_f_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader for merged dataset (hard negative sampling)\n",
    "train_evcls_hard_f_set = EVCLSDataset(train_evcls_hard_f, token_len)\n",
    "train_evcls_hard_f_loader = DataLoader(train_evcls_hard_f_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sturcture for model of evidence classification\n",
    "class EvidenceClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(EvidenceClassifier, self).__init__()\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg_ids):\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, token_type_ids = seg_ids, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy for model of evidence classification\n",
    "def acc_sigmoid(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "# Evaluate model performance using dev set\n",
    "# acc_func: accuracy function (sigmoid or softmax)\n",
    "# cls_type: 0 for evidence classfication, 1 for claim classification\n",
    "def evaluate(model, criterion, devloader, acc_func, cls_type=0):\n",
    "    model.eval()\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, seg_ids, labels in devloader:\n",
    "            seq, attn_masks, seg_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids.cuda(gpu), labels.cuda(gpu)\n",
    "            logits = model(seq, attn_masks, seg_ids)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item() if cls_type == 0 else criterion(logits, labels).item()\n",
    "            mean_acc += acc_func(logits, labels)\n",
    "            count += 1\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encaplusate the training process\n",
    "# acc_func: accuracy function (sigmoid or softmax)\n",
    "# cls_type: 0 for evidence classfication, 1 for claim classification\n",
    "# full: True for using the merged dataset\n",
    "# Based on the code from week 7's workshop\n",
    "def train(model, criterion, optimizer, train_loader, dev_loader, max_eps, acc_func, name, cls_type=0, full=False):\n",
    "    if full:\n",
    "        print(\"Using merged dataset ...\")\n",
    "\n",
    "    best_acc = 0\n",
    "    st = time.time()\n",
    "    for ep in range(max_eps):\n",
    "        print(f\"Epoch {ep} ...\")\n",
    "        model.train()\n",
    "        for i, (seq, attn_masks, seg_ids, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()  \n",
    "            seq, attn_masks, seg_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids.cuda(gpu), labels.cuda(gpu)\n",
    "            logits = model(seq, attn_masks, seg_ids)\n",
    "            loss = criterion(logits.squeeze(-1), labels.float()) if cls_type == 0 else criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                acc = acc_func(logits, labels)\n",
    "                print(f\"Iteration {i} of epoch {ep} complete. Loss: {loss.item()}; Accuracy: {acc}; Time: {round((time.time() - st), 2)}s\")\n",
    "                st = time.time()\n",
    "\n",
    "        if not full:\n",
    "            # Evaluate the model using the dev set when the epoch is finished\n",
    "            dev_acc, dev_loss = evaluate(model, criterion, dev_loader, acc_func, cls_type=cls_type)\n",
    "            print(f\"\\nEpoch {ep} completed. Development Accuracy: {dev_acc}; Development Loss: {dev_loss}\\n\")\n",
    "            # Save the model if accuracy is improved\n",
    "            if dev_acc > best_acc:\n",
    "                print(f\"Best accuracy is improved from {best_acc} to {dev_acc}\")\n",
    "                best_acc = dev_acc\n",
    "                torch.save(model.state_dict(), f\"{outputs_path}{name}.dat\")\n",
    "                print(f\"Model is saved to {outputs_path}{name}.dat\\n\")\n",
    "        else:\n",
    "            torch.save(model.state_dict(), f\"{outputs_path}{name}.dat\")\n",
    "            print(f\"Model is saved to {outputs_path}{name}.dat\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/evcls.dat exists\n"
     ]
    }
   ],
   "source": [
    "# Train: random sampling\n",
    "evcls_name = \"evcls\"\n",
    "if not check_file(f\"{outputs_path}{evcls_name}.dat\"):\n",
    "    evcls_model = EvidenceClassifier()\n",
    "    evcls_model.to(device)\n",
    "    evcls_criterion = nn.BCEWithLogitsLoss()\n",
    "    evcls_optimizer = optim.Adam(evcls_model.parameters(), lr=2e-5)\n",
    "    num_epoch = 2\n",
    "    train(evcls_model, evcls_criterion, evcls_optimizer, train_evcls_loader, dev_evcls_loader, num_epoch, acc_sigmoid, evcls_name)\n",
    "else:\n",
    "    print(f\"{outputs_path}{evcls_name}.dat exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/evcls_hard.dat exists\n"
     ]
    }
   ],
   "source": [
    "# Train: hard negative sampling\n",
    "evcls_hard_name = \"evcls_hard\"\n",
    "if not check_file(f\"{outputs_path}{evcls_hard_name}.dat\"):\n",
    "    evcls_hard_model = EvidenceClassifier()\n",
    "    evcls_hard_model.to(device)\n",
    "    evcls_hard_criterion = nn.BCEWithLogitsLoss()\n",
    "    evcls_hard_optimizer = optim.Adam(evcls_hard_model.parameters(), lr=2e-5)\n",
    "    num_epoch = 2\n",
    "    train(evcls_hard_model, evcls_hard_criterion, evcls_hard_optimizer, train_evcls_hard_loader, dev_evcls_hard_loader, num_epoch, acc_sigmoid, evcls_hard_name)\n",
    "else:\n",
    "    print(f\"{outputs_path}{evcls_hard_name}.dat exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/evcls_f.dat exists\n"
     ]
    }
   ],
   "source": [
    "# Train: random sampling (merged)\n",
    "evcls_f_name = \"evcls_f\"\n",
    "if not check_file(f\"{outputs_path}{evcls_f_name}.dat\"):\n",
    "    evcls_f_model = EvidenceClassifier()\n",
    "    evcls_f_model.to(device)\n",
    "    evcls_f_criterion = nn.BCEWithLogitsLoss()\n",
    "    evcls_f_optimizer = optim.Adam(evcls_f_model.parameters(), lr=2e-5)\n",
    "    num_epoch = 1\n",
    "    train(evcls_f_model, evcls_f_criterion, evcls_f_optimizer, train_evcls_f_loader, None, num_epoch, acc_sigmoid, evcls_f_name, full=True)\n",
    "else:\n",
    "    print(f\"{outputs_path}{evcls_f_name}.dat exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/evcls_hard_f.dat exists\n"
     ]
    }
   ],
   "source": [
    "# Train: hard negative sampling (merged)\n",
    "evcls_hard_f_name = \"evcls_hard_f\"\n",
    "if not check_file(f\"{outputs_path}{evcls_hard_f_name}.dat\"):\n",
    "    evcls_hard_f_model = EvidenceClassifier()\n",
    "    evcls_hard_f_model.to(device)\n",
    "    evcls_hard_f_criterion = nn.BCEWithLogitsLoss()\n",
    "    evcls_hard_f_optimizer = optim.Adam(evcls_hard_f_model.parameters(), lr=2e-5)\n",
    "    num_epoch = 1\n",
    "    train(evcls_hard_f_model, evcls_hard_f_criterion, evcls_hard_f_optimizer, train_evcls_hard_f_loader, None, num_epoch, acc_sigmoid, evcls_hard_f_name, full=True)\n",
    "else:\n",
    "    print(f\"{outputs_path}{evcls_hard_f_name}.dat exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/evcls.dat\n"
     ]
    }
   ],
   "source": [
    "# Load: random sampling\n",
    "evcls_path = f\"{outputs_path}{evcls_name}.dat\"\n",
    "evcls_model = EvidenceClassifier()\n",
    "evcls_model.load_state_dict(torch.load(evcls_path))\n",
    "evcls_model.to(device)\n",
    "evcls_model.eval()\n",
    "print(f\"Loaded {evcls_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/evcls_hard.dat\n"
     ]
    }
   ],
   "source": [
    "# Load: hard negative sampling\n",
    "evcls_hard_path = f\"{outputs_path}{evcls_hard_name}.dat\"\n",
    "evcls_hard_model = EvidenceClassifier()\n",
    "evcls_hard_model.load_state_dict(torch.load(evcls_hard_path))\n",
    "evcls_hard_model.to(device)\n",
    "evcls_hard_model.eval()\n",
    "print(f\"Loaded {evcls_hard_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/evcls_f.dat\n"
     ]
    }
   ],
   "source": [
    "# Load: random sampling (merged)\n",
    "evcls_f_path = f\"{outputs_path}{evcls_f_name}.dat\"\n",
    "evcls_f_model = EvidenceClassifier()\n",
    "evcls_f_model.load_state_dict(torch.load(evcls_f_path))\n",
    "evcls_f_model.to(device)\n",
    "evcls_f_model.eval()\n",
    "print(f\"Loaded {evcls_f_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/evcls_hard_f.dat\n"
     ]
    }
   ],
   "source": [
    "# Load: hard negative sampling (merged)\n",
    "evcls_hard_f_path = f\"{outputs_path}{evcls_hard_f_name}.dat\"\n",
    "evcls_hard_f_model = EvidenceClassifier()\n",
    "evcls_hard_f_model.load_state_dict(torch.load(evcls_hard_f_path))\n",
    "evcls_hard_f_model.to(device)\n",
    "evcls_hard_f_model.eval()\n",
    "print(f\"Loaded {evcls_hard_f_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evidence retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify if the evidence is relevant to the claim\n",
    "def evcls(claim, evidence, model, tokenizer):\n",
    "    seq, attn_mask, seg_id = convert_input(claim, evidence, tokenizer, token_len)\n",
    "    seq = seq.unsqueeze(0).cuda(gpu)\n",
    "    attn_mask = attn_mask.unsqueeze(0).cuda(gpu)\n",
    "    seg_id = seg_id.unsqueeze(0).cuda(gpu)\n",
    "    with torch.no_grad():\n",
    "        return torch.sigmoid(model(seq, attn_mask, seg_id))[0][0]\n",
    "\n",
    "# Retrieve evidences for a claim\n",
    "# n: number of candidates to retrieve\n",
    "# top: number of evidences to extract from candidates\n",
    "# model: model to use (random sampling)\n",
    "# hard_model: model to use (hard negative sampling)\n",
    "def ev_retrieve(claim, model, hard_model, n=100, top=5):\n",
    "    # Extract evidences which have high similarity with the claim\n",
    "    candidates = tfidf_filter(claim, n)\n",
    "    claim_emd = get_text_embedding([claim], distilbert_model, distilbert_tokenizer, show=False)[0]\n",
    "    claim_clean = text_clean(claim)\n",
    "    \n",
    "    res = []\n",
    "    for candidate in candidates:\n",
    "        ev_id = candidate[0]\n",
    "        ev = evidence_src[ev_id]\n",
    "        ev_clean = clean_evidence_src[ev_id]\n",
    "\n",
    "        tfidf = candidate[1]\n",
    "        jaccard = jaccard_similarity(claim_clean, ev_clean)\n",
    "        distil_sim = cosine_similarity([claim_emd], [ev_embeddings[ev_id]]).item()\n",
    "        pred = evcls(claim, ev, model=model, tokenizer=bert_tokenizer).item()\n",
    "        hard_pred = evcls(claim, ev, model=hard_model, tokenizer=bert_tokenizer).item()\n",
    "        \n",
    "        score = 0.05 * tfidf + 0.05 * jaccard + 0.2 * distil_sim + 0.4 * pred + 0.3 * hard_pred\n",
    "        res.append((ev_id, score))\n",
    "    res = sorted(res, key = lambda x: x[1], reverse=True)\n",
    "    return res[:top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaulate the performance of evidence retrieval on a claim in dev set\n",
    "# show; True for printing stats\n",
    "# full: True for using models trained on merged dataset\n",
    "def eval_dev_evcls(index, n=100, top=5, show=True, full=False):\n",
    "    # Select model\n",
    "    model = evcls_f_model if full else evcls_model\n",
    "    hard_model = evcls_hard_f_model if full else evcls_hard_model\n",
    "    \n",
    "    claim = dev_claims[index]\n",
    "    truths = dev_evidences[index]\n",
    "    evidences = [item[0] for item in ev_retrieve(claim, model, hard_model, n, top)]\n",
    "    print(evidences) if show else None\n",
    "    t = 0\n",
    "    f = 0\n",
    "    for truth in truths:\n",
    "        if int(truth[9:]) in evidences:\n",
    "            t += 1\n",
    "            print(f\"In: {truth}\") if show else None\n",
    "        else:\n",
    "            f += 1\n",
    "            print(f\"Out: {truth}\") if show else None\n",
    "    print(f\"In: {t}, Out: {f}\") if show else None\n",
    "    return t, f\n",
    "\n",
    "# Retrieve evidences for all claims from a dataset\n",
    "# check: True for checking number of correct retrieved evidences\n",
    "# full: True for using models trained on merged dataset\n",
    "def ev_retrieve_src(datasrc, n=100, top=2, check=False, full=False):\n",
    "    # Select model\n",
    "    model = evcls_f_model if full else evcls_model\n",
    "    hard_model = evcls_hard_f_model if full else evcls_hard_model\n",
    "\n",
    "    evidences = []\n",
    "    t = 0\n",
    "    f = 0\n",
    "    pbar = tqdm(total=len(datasrc), desc=\"Retrieving evidences\", dynamic_ncols=True)\n",
    "    for i in range(len(datasrc)):\n",
    "        claim = datasrc[i]\n",
    "        evidences.append([item[0] for item in ev_retrieve(claim, model, hard_model, n, top)])\n",
    "        if check:\n",
    "            truths = dev_evidences[i]\n",
    "            for truth in truths:\n",
    "                if int(truth[9:]) in evidences[-1]:\n",
    "                    t += 1\n",
    "                else:\n",
    "                    f += 1\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    print(f\"Total: {t + f}, In: {t}, Out: {f}\") if check else None\n",
    "    return evidences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section II: Claim Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {'SUPPORTS': 0, 'REFUTES': 1, 'NOT_ENOUGH_INFO': 2, 'DISPUTED': 3}\n",
    "id2label = {0: 'SUPPORTS', 1: 'REFUTES', 2: 'NOT_ENOUGH_INFO', 3: 'DISPUTED'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT for claim classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data for claim classification\n",
    "def create_claimcls_data(claims, evs, labels):\n",
    "    claim_list = []\n",
    "    ev_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for i, claim in enumerate(claims):\n",
    "        ev_ids = evs[i]\n",
    "        for num in ev_ids:\n",
    "            ev_id = int(num[9:])\n",
    "            evidence_text = evidence_src[ev_id]\n",
    "            claim_list.append(claim)\n",
    "            ev_list.append(evidence_text)\n",
    "            label_list.append(labels[i])\n",
    "\n",
    "    return pd.DataFrame({'Claim': claim_list, 'Evidence': ev_list, 'Label': label_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and development datasets for claim classification\n",
    "train_claimcls = create_claimcls_data(train_claims, train_evidences, train_labels)\n",
    "dev_claimcls = create_claimcls_data(dev_claims, dev_evidences, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge training and development datasets for claim classification\n",
    "train_claimcls_f = pd.merge(train_claimcls, dev_claimcls, on=['Claim', 'Evidence', 'Label'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for claim classification\n",
    "class ClaimCLSDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, maxlen):\n",
    "        self.df = data\n",
    "        self.tokenizer = bert_tokenizer\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        claim = self.df.loc[index, 'Claim']\n",
    "        evidence = self.df.loc[index, 'Evidence']\n",
    "        label = torch.tensor(label2id[self.df.loc[index, 'Label']])\n",
    "\n",
    "        tokens_ids_t, attn_mask_t, seg_ids_t = convert_input(claim, evidence, self.tokenizer, self.maxlen)\n",
    "        return tokens_ids_t, attn_mask_t, seg_ids_t, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and development dataloaders for claim classification\n",
    "train_claimcls_set = ClaimCLSDataset(train_claimcls, token_len)\n",
    "dev_claimcls_set = ClaimCLSDataset(dev_claimcls, token_len)\n",
    "train_claimcls_loader = DataLoader(train_claimcls_set, batch_size = 32, shuffle=True)\n",
    "dev_claimcls_loader = DataLoader(dev_claimcls_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader for merged dataset\n",
    "train_claimcls_f_set = ClaimCLSDataset(train_claimcls_f, token_len)\n",
    "train_claimcls_f_loader = DataLoader(train_claimcls_f_set, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sturcture of the model for Claim classification\n",
    "class ClaimClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ClaimClassifier, self).__init__()\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.cls_layer = nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, seq, attn_masks, seg_ids):\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks, token_type_ids = seg_ids, return_dict=True)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy for model of claim classification\n",
    "def acc_softmax(logits, labels):\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    _, pred_labels = torch.max(probs, dim=1)\n",
    "    total = labels.size(0)\n",
    "    correct = torch.sum(pred_labels == labels).item()\n",
    "    acc = correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/claimcls.dat exists\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "claimcls_name = \"claimcls\"\n",
    "if not check_file(f\"{outputs_path}{claimcls_name}.dat\"):\n",
    "    claimcls_model = ClaimClassifier()\n",
    "    claimcls_model.to(device)\n",
    "    claimcls_criterion = nn.CrossEntropyLoss()\n",
    "    claimcls_optimizer = optim.Adam(claimcls_model.parameters(), lr=2e-5)\n",
    "    num_epoch = 2\n",
    "    train(claimcls_model, claimcls_criterion, claimcls_optimizer, train_claimcls_loader, dev_claimcls_loader, num_epoch, acc_softmax, claimcls_name, cls_type=1)\n",
    "else:\n",
    "    print(f\"{outputs_path}{claimcls_name}.dat exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/claimcls_f.dat exists\n"
     ]
    }
   ],
   "source": [
    "# Train on merged dataset\n",
    "claimcls_f_name = \"claimcls_f\"\n",
    "if not check_file(f\"{outputs_path}{claimcls_f_name}.dat\"):\n",
    "    claimcls_f_model = ClaimClassifier()\n",
    "    claimcls_f_model.to(device)\n",
    "    claimcls_f_criterion = nn.CrossEntropyLoss()\n",
    "    claimcls_f_optimizer = optim.Adam(claimcls_f_model.parameters(), lr=2e-5)\n",
    "    num_epoch = 1\n",
    "    train(claimcls_f_model, claimcls_f_criterion, claimcls_f_optimizer, train_claimcls_f_loader, None, num_epoch, acc_softmax, claimcls_f_name, cls_type=1, full=True)\n",
    "else:\n",
    "    print(f\"{outputs_path}{claimcls_f_name}.dat exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/claimcls.dat\n"
     ]
    }
   ],
   "source": [
    "# Load model for claim classification\n",
    "claimcls_path = f\"{outputs_path}{claimcls_name}.dat\"\n",
    "claimcls_model = ClaimClassifier()\n",
    "claimcls_model.load_state_dict(torch.load(claimcls_path))\n",
    "claimcls_model.to(device)\n",
    "claimcls_model.eval()\n",
    "print(f\"Loaded {claimcls_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded outputs/claimcls_f.dat\n"
     ]
    }
   ],
   "source": [
    "# Load model for claim classification (merged dataset)\n",
    "claimcls_f_path = f\"{outputs_path}{claimcls_f_name}.dat\"\n",
    "claimcls_f_model = ClaimClassifier()\n",
    "claimcls_f_model.load_state_dict(torch.load(claimcls_f_path))\n",
    "claimcls_f_model.to(device)\n",
    "claimcls_f_model.eval()\n",
    "print(f\"Loaded {claimcls_f_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claim classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a claim\n",
    "def claimcls(claim, evidences, model, tokenizer):\n",
    "    res = []\n",
    "    for ev in evidences:\n",
    "        seq, attn_mask, seg_id = convert_input(claim, evidence_src[ev], tokenizer, token_len)\n",
    "        seq = seq.unsqueeze(0).cuda(gpu)\n",
    "        attn_mask = attn_mask.unsqueeze(0).cuda(gpu)\n",
    "        seg_id = seg_id.unsqueeze(0).cuda(gpu)\n",
    "        with torch.no_grad():\n",
    "            logits = model(seq, attn_mask, seg_id)\n",
    "            _, pred = torch.max(torch.softmax(logits, dim=1), dim=1)\n",
    "            pred = pred.item()\n",
    "            if pred != 2:\n",
    "                res.append(pred)\n",
    "    if len(res) == 0:\n",
    "        return 2\n",
    "    return Counter(res).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on a claim in dev set\n",
    "# show: True for printing predicton and truth\n",
    "def eval_dev_claimcls(index, ev_retrieve, model, tokenizer, show=True):\n",
    "    pred = claimcls(dev_claims[index], ev_retrieve[index], model, tokenizer)\n",
    "    truth = label2id[dev_labels[index]]\n",
    "    print(f\"Pred: {pred}, Truth: {truth}\") if show else None\n",
    "    return pred\n",
    "\n",
    "# Classify all claims in the dataset based on the retrived evidences\n",
    "# check: True for checking accuracy\n",
    "def claimcls_src(claims, evs, model, tokenizer, check=False):\n",
    "    preds = []\n",
    "    t = 0\n",
    "    f = 0\n",
    "    pbar = tqdm(total=len(claims), desc=\"Predicting claims\", dynamic_ncols=True)\n",
    "    for i in range(len(claims)):\n",
    "        pred = claimcls(claims[i], evs[i], model, tokenizer)\n",
    "        preds.append(pred)\n",
    "        if check:\n",
    "            truth = label2id[dev_labels[i]]\n",
    "            if pred == truth:\n",
    "                t += 1\n",
    "            else:\n",
    "                f += 1\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    print(f\"Total: {t + f}, Correct: {t}, Wrong: {f}, Accuracy: {round(t / (t + f), 2)}\") if check else None\n",
    "    return preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section III: Evaluation & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format evidences for saving\n",
    "def format_evidences(evidences):\n",
    "    res = []\n",
    "    for ev in evidences:\n",
    "        res.append(f\"evidence-{ev}\")\n",
    "    return res\n",
    "\n",
    "# Formatting and saving results\n",
    "def save_results(claim_ids, claims, labels, evidences, filename):\n",
    "    data = {}\n",
    "    for i in range(len(claim_ids)):\n",
    "        claim_id = claim_ids[i]\n",
    "        claim_text = claims[i]\n",
    "        claim_label = id2label[labels[i]]\n",
    "        evs = format_evidences(evidences[i])\n",
    "        data[claim_id] = {\"claim_text\": claim_text,\n",
    "                          \"claim_label\": claim_label,\n",
    "                          \"evidences\": evs}\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34981439ee4488395a8e036fa843459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving evidences:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 491, In: 120, Out: 371\n"
     ]
    }
   ],
   "source": [
    "# Retrieve evidences for claims in dev set\n",
    "dev_ev_retrieve = ev_retrieve_src(dev_claims, 150, 5, check=True, full=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b259aae26f9744aca5e468359f0aa098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting claims:   0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 154, Correct: 71, Wrong: 83, Accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "# Classify claims in dev set\n",
    "dev_claimcls = claimcls_src(dev_claims, dev_ev_retrieve, claimcls_model, bert_tokenizer, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to prediction/dev-pred.json\n"
     ]
    }
   ],
   "source": [
    "dev_pred_output = \"dev-pred.json\"\n",
    "dev_save_path = f\"{prediction_path}{dev_pred_output}\"\n",
    "save_results(dev_claim_ids, dev_claims, dev_claimcls, dev_ev_retrieve, dev_save_path)\n",
    "print(f\"Saved to {dev_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidence Retrieval F-score (F)    = 0.18772933415790563\n",
      "Claim Classification Accuracy (A) = 0.461038961038961\n",
      "Harmonic Mean of F and A          = 0.26681494091949504\n"
     ]
    }
   ],
   "source": [
    "if check_file(\"eval.py\"):\n",
    "    !python eval.py --predictions data/dev-claims.json --groundtruth prediction/dev-pred.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = f\"{data_dir}test-claims-unlabelled.json\"\n",
    "test_claim_ids, test_claims = load_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91ad4fd075a47b780e17643668fb897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Retrieving evidences:   0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve evidences for claims in test set\n",
    "test_ev_retrieve = ev_retrieve_src(test_claims, 150, 5, full=False)\n",
    "# test_ev_retrieve = ev_retrieve_src(test_claims, 150, 5, full=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed1692cc4084902afac9c674bb20b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting claims:   0%|          | 0/153 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Classify claims in test set\n",
    "test_claimcls = claimcls_src(test_claims, test_ev_retrieve, claimcls_model, bert_tokenizer)\n",
    "# test_claimcls = claimcls_src(test_claims, test_ev_retrieve, claimcls_f_model, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to prediction/test-claims-predictions.json\n"
     ]
    }
   ],
   "source": [
    "test_pred_output = \"test-claims-predictions.json\"\n",
    "test_save_path = f\"{prediction_path}{test_pred_output}\"\n",
    "save_results(test_claim_ids, test_claims, test_claimcls, test_ev_retrieve, test_save_path)\n",
    "print(f\"Saved to {test_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_insight(index):\n",
    "    print(f\"Claim: {test_claims[index]}\")\n",
    "    for ev in test_ev_retrieve[index]:\n",
    "        print(evidence_src[ev])\n",
    "    print(f\"Prediction: {id2label[test_claimcls[index]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim: The contribution of waste heat to the global climate is 0.028 W/m2.\n",
      "Global forcing from waste heat was 0.028 W/m2 in 2005.\n",
      "The global temperature increase since the beginning of the industrial period (taken as 1750) is about 0.8 °C (1.4 °F), and the radiative forcing due to CO 2 and other long-lived greenhouse gases – mainly methane, nitrous oxide, and chlorofluorocarbons – emitted since that time is about 2.6 W/m2.\n",
      "Taking planetary heat uptake rate as the rate of ocean heat uptake estimated by the IPCC AR4 as 0.2 W/m2, yields a value for S of 2.1 °C (3.8 °F).\n",
      "Without feedbacks the radiative forcing of approximately 3.7 W/m2, due to doubling CO 2 from the pre-industrial 280 ppm, would eventually result in roughly 1 °C global warming.\n",
      "Solar irradiance is about 0.9 W/m2 brighter during solar maximum than during solar minimum, which correlated in measured average global temperature over the period 1959-2004.\n",
      "Prediction: SUPPORTS\n"
     ]
    }
   ],
   "source": [
    "test_insight(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
